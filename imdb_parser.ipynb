{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, filename='debug.log', format='[%(asctime)s] %(levelname)s:%(message)s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "imdb = 'https://www.imdb.com'\n",
    "\n",
    "params = {\n",
    "    'title_type' : 'feature', #feature, tv_movie, tv_series, tv_episode, tv_special, tv_miniseries, tv_miniseries, documentary, video_game, short, video, tv_short\n",
    "    'release_date-min' : '',#'2019-10-01', #Format: YYYY-MM-DD, YYYY-MM, or YYYY\n",
    "    'release_date-max' : '2019-10-25', #Format: YYYY-MM-DD, YYYY-MM, or YYYY\n",
    "    'genres' : [], #action adventure animation biography comedy crime documentary drama family fantasy film_noir game_show history horror music musical mystery news reality_tv romance sci_fi sport talk_show thriller war western\n",
    "    'user_rating-min' : '1', #1.0 to 10.0\n",
    "    'user_rating-max' : '',#'7.0', #1.0 to 10.0\n",
    "    'countries' : ['ru'],\n",
    "    'count' : '250', #Number of films per page\n",
    "    'start' : '1',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    \"accept-language\": \"ru-RU,ru;q=0.9\",\n",
    "    \"content-type\": \"application/x-www-form-urlencoded\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Парсинг всех фильмов на странице\n",
    "\n",
    "def parse_one_page(soup):\n",
    "\n",
    "    films_soup = soup.findAll('div', class_=\"lister-item-image float-left\")\n",
    "    films_parsed = 0\n",
    "\n",
    "    res = {}\n",
    "\n",
    "    for film in films_soup:\n",
    "        try:\n",
    "            film_dict = {}\n",
    "\n",
    "            films_parsed += 1\n",
    "\n",
    "            film_link = film.a.get('href')\n",
    "            film_id = film_link.split('/')[-2]\n",
    "\n",
    "            film_dict['title_link'] = imdb + film_link\n",
    "\n",
    "            film_name = film.img.get('alt')\n",
    "            film_dict['title_name'] = film_name\n",
    "\n",
    "            film_html = requests.post(imdb + film_link, headers=headers)\n",
    "\n",
    "            if not film_html.ok:\n",
    "                retry_number = 0\n",
    "                while film_html.ok == False or retry_number != 4:\n",
    "                    time.sleep(5)\n",
    "                    logging.error(f'Error in page loading, retry №{retry_number}')\n",
    "                    film_html = requests.post(imdb + film_link, headers=headers)\n",
    "                    retry_number += 1\n",
    "\n",
    "            else:\n",
    "                logging.info(f'Successfully loaded page id={film_id}')\n",
    "\n",
    "            film_soup = BeautifulSoup(film_html.text, 'html.parser')\n",
    "            json_film_soup = film_soup.find(\"script\", type=\"application/ld+json\")\n",
    "\n",
    "            if json_film_soup is not None:\n",
    "                json_film = json.loads(json_film_soup.string)\n",
    "\n",
    "\n",
    "                try:\n",
    "                    film_dict['genre'] = json_film['genre']\n",
    "                    logging.info(f'Successfully added genre')\n",
    "\n",
    "                except Exception as exception:\n",
    "                    logging.error(f'Genre not found {exception}')\n",
    "\n",
    "\n",
    "                try:\n",
    "                    film_dict['rating'] = json_film['aggregateRating']['ratingValue']\n",
    "                    logging.info(f'Successfully added rating')\n",
    "\n",
    "                except Exception as exception:\n",
    "                    logging.error(f'Rating not found {exception}')\n",
    "\n",
    "\n",
    "                try:\n",
    "                    actors = []\n",
    "                    for actor in json_film['actor']:\n",
    "                        actors.append(actor['name'])\n",
    "\n",
    "                    film_dict['actors'] = actors\n",
    "                    logging.info(f'Successfully added actors')\n",
    "\n",
    "                except Exception as exception:\n",
    "                    logging.error(f'Actors not found {exception}')\n",
    "\n",
    "\n",
    "                try:\n",
    "                    film_dict['type']  = json_film['@type']\n",
    "                    logging.info(f'Successfully added type')\n",
    "\n",
    "                except Exception as exception:\n",
    "                    logging.error(f'Type not found {exception}')\n",
    "\n",
    "            else:\n",
    "                logging.error('Json not found')\n",
    "\n",
    "            film_details_soup = film_soup.find('div', id='titleDetails')\n",
    "\n",
    "            # Ссылки на официальные сайты очень длинные, но этот кусок кода работает\n",
    "            # if film_details_soup is not None:\n",
    "            #     try:\n",
    "            #         official_sites_soup = film_details_soup.find('h4', text='Official Sites:')\n",
    "            #         official_sites_dirty = official_sites_soup.find_next_siblings('a')\n",
    "            #         official_sites = []\n",
    "            #\n",
    "            #         for site in official_sites_dirty:\n",
    "            #             official_sites.append(imdb + site['href'])\n",
    "            #\n",
    "            #         film_dict['official_sites'] = official_sites\n",
    "            #\n",
    "            #         logging.info('Successfully added official sites ')\n",
    "            #\n",
    "            #     except Exception as exception:\n",
    "            #         logging.error('Official sites not loaded {exception}')\n",
    "            #\n",
    "            # else:\n",
    "            #     logging.error('Official sites not found')\n",
    "\n",
    "            if film_details_soup is not None:\n",
    "                for child in film_details_soup.children:\n",
    "                    if child.name == 'h2' or child.name == 'h3':\n",
    "                        cell_title = child.text.strip()\n",
    "                        cell_is_good = cell_title not in 'Company Credits'\n",
    "\n",
    "                        if cell_is_good: film_dict[cell_title] = {}\n",
    "\n",
    "                    if child.name == 'div' and 'txt-block' in child['class'] and cell_is_good:\n",
    "                        key_word = child.find('h4')\n",
    "\n",
    "                        if key_word is not None:\n",
    "                            good_key_word = key_word.text.strip(':')\n",
    "                            child.h4.decompose()\n",
    "                            details_value = []\n",
    "\n",
    "                            for child_text in child.text.replace('See more\\xa0»', '').strip().split('|'):\n",
    "                                details_value.append(re.sub(r'\\n', '', child_text).strip())\n",
    "\n",
    "                            if len(details_value) == 1:\n",
    "                                film_dict[cell_title][good_key_word] = details_value.pop()\n",
    "\n",
    "                            else:\n",
    "                                film_dict[cell_title][good_key_word] = details_value\n",
    "\n",
    "                logging.info('Successfully added details block')\n",
    "\n",
    "            else:\n",
    "                logging.error('Details block not found')\n",
    "\n",
    "            res[film_id] = film_dict\n",
    "            # print(film_dict)\n",
    "\n",
    "        except Exception as exception:\n",
    "            logging.error(f'Failed parsing of this title {exception}')\n",
    "\n",
    "    return res, films_parsed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Поиск с заданными вверху параметрами\n",
    "\n",
    "def load_html_list(head=headers, par=params):\n",
    "    html_page = requests.post(imdb + '/search/title/', headers=head, params=par)\n",
    "    if html_page.ok:\n",
    "        logging.info(f'Successfully got html from page')\n",
    "        return BeautifulSoup(html_page.text, 'html.parser')\n",
    "    else:\n",
    "        logging.error(f'Failed to get html from page')\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 250 of 2696 titles\n",
      "Parsed 500 of 2696 titles\n",
      "Parsed 750 of 2696 titles\n",
      "Parsed 1000 of 2696 titles\n"
     ]
    }
   ],
   "source": [
    "# Что-то вроде Main\n",
    "\n",
    "logging.info(f'Started parsing')\n",
    "\n",
    "soup = load_html_list()\n",
    "\n",
    "if soup is not None:\n",
    "    number_of_films_soup = soup.find('div', class_='desc')\n",
    "    number_of_films = max([int(el) for el in number_of_films_soup.text.replace(',', '').split() if el.isdigit()])\n",
    "\n",
    "    res_dict, films_parsed = parse_one_page(soup)\n",
    "\n",
    "    all_res = [res_dict]\n",
    "    print(f'Parsed {films_parsed} of {number_of_films} titles')\n",
    "\n",
    "    if films_parsed < number_of_films:\n",
    "        for i in range(1, 4):\n",
    "            params[\"start\"] = str(250 * i + 1)\n",
    "\n",
    "            soup = load_html_list()\n",
    "\n",
    "            if soup is not None:\n",
    "                tmp_res_dict, tmp_films_parsed = parse_one_page(soup)\n",
    "\n",
    "                films_parsed += tmp_films_parsed\n",
    "\n",
    "                logging.info(f'Parsed {films_parsed} films')\n",
    "                all_res.append(tmp_res_dict)\n",
    "                print(f'Parsed {films_parsed} of {number_of_films} titles')\n",
    "\n",
    "                if films_parsed == number_of_films:\n",
    "                    break\n",
    "\n",
    "            else:\n",
    "                logging.error(f'Failed to load')\n",
    "\n",
    "    logging.info(f'Parsing finished')\n",
    "\n",
    "else:\n",
    "    logging.info(f'Parsing failed, stopping')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Сохранение в файл\n",
    "\n",
    "try:\n",
    "    with open('parsedTitles.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(all_res, json_file, ensure_ascii=False)\n",
    "        logging.info('Json file created')\n",
    "\n",
    "except Exception as exception:\n",
    "    logging.error(f'Json file failed to create {exception}')\n",
    "\n",
    "# Дальше идут ячейки, которые я использовал для отладки, они не важны"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for res in all_res:\n",
    "    print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "html = soup.prettify()\n",
    "with open(\"out.html\",\"w\", encoding='utf-8') as out:\n",
    "    for i in range(0, len(html)):\n",
    "        try:\n",
    "            out.write(html[i])\n",
    "        except Exception:\n",
    "            pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "url = './out.html'\n",
    "\n",
    "# Windows\n",
    "chrome_path = 'C:/Program Files/Google/Chrome/Application/chrome.exe %s'\n",
    "\n",
    "webbrowser.get(chrome_path).open(url)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}